<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PECC</title>

  <meta property="og:title" content="PECC" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://lm-pub-quiz.github.io/" />
  <!-- <meta property="og:image" content="https://lm-pub-quiz.github.io/media/preview.png" /> -->

  <link rel="stylesheet" href="https://fonts.xz.style/serve/inter.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark-dimmed.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

<script>hljs.highlightAll();</script>
</head>
<body>
    <header>
      <div class="title-div">
        <h1><span class="title-lm">PECC</span><br><span class="title-pq"></span></h1>
      </div>
      <div class="subtitle-div">
        <h2 class="subtitle">Problem Extraction and Coding Challenges</h2>
        <nav>
            <a href="https://github.com/HallerPatrick/pecc"><i class="bi bi-git"></i>&nbsp;Library</a> /
            <a href="https://huggingface.co/spaces/PatrickHaller/pecc-leaderboard"<i class="bi bi-bar-chart"></i>&nbsp;Leaderboard</a> /
            <a href="https://huggingface.co/datasets/PatrickHaller/pecc"><i class="bi bi-database"></i>&nbsp;PECC Dataset</a> /
            <a href=""><i class="bi bi-file-earmark"></i>&nbsp;Paper</a> <!-- /
            <a href=""><i class="bi bi-file-zip"></i>&nbsp;Raw Results</a> -->
        </nav>
      </div>
    </header>

    <section>
      <figure class="shadow-box">
        <!-- <img src="./media/bear_evaluation_final.svg" width="100%" alt="Illustration of how LM Pub Quiz evaluates LMs."> -->
        <figcaption>Illustration of how LM Pub Quiz evaluates LMs: Answers are ranked by the (pseudo) log-likelihoods of the textual statements derived from all of the answer options.</figcaption> 
      </figure>
    </section>
    <section class="shadow-box">
      <h2>Leaderboard</h2>
      <p>
        <!-- We evaluated 22 lanuages models (of various sizes, trained using different pretraining objectives, and of both causal and masked LM types) on the BEAR dataset. -->
        To this point, we evaluated 7 competitive large language models. Depending on instruction or chat-finetuning of some models a whole evaluation on the AoC dataset subset was not possible.
        We average the accuracy over all 4 subsets.
      </p>
      <div style="overflow-x: scroll;">

<table border="1" class="leaderboard">
  <colgroup>
     <col span="1" style="min-width: 4rem;">
     <col span="1" style="min-width: 3rem;">
     <col span="1" style="min-width: 4rem;">
     <col span="1" style="min-width: 7rem;">
  </colgroup>
  <thead>
    <tr style="text-align: left;">
      <th>Model</th>
      <th>Num Params</th>
      <th>PECC (Pass@3)</th>
    </tr>
  </thead>
  <tbody>


    <tr>
      <td>Claude Haiku</td>
      <td>-</td>
      <td>&#8199;27.67%</td>
    </tr>
    <tr>
      <td>GPT-3.5-Turbo</td>
      <td>-</td>
      <td>&#8199;23.75%</td>
    </tr>
    <tr>
      <td>Codechat-Bison</td>
      <td>-</td>
      <td>&#8199;11.39%</td>
    </tr>
    <tr>
      <td>Chat-Bison</td>
      <td>-</td>
      <td>&#8199;8.48%</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">Mixtral-8x7B-Instruct-v0.1</a></td>
      <td>56B</td>
      <td>&#8199;8.36%</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0">WizardCoder-Python-34B-V1.0</a></td>
      <td>34b</td>
      <td>&#8199;12.9%</td>
    </tr>
    <tr>
        <td><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1">Mistral-7B-Instruct-v0.1</a></td>
        <td>7B</td>
        <td>&#8199;1.62%</td>
    </tr>
  </tbody>
</table>
      </div>
    </section>
    <section class="shadow-box">
      <div style="text-align: right;"><span class="badge acl">Accepted at LREC-COLING 2024</span></div>
      <h2>PECC: Problem Extraction and Coding Challenges</h2>
      <h3>Abstract</h3>
      <p>
      Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various
tasks, such as code generation, problem-solving and reasoning. Existing benchmarks evaluate tasks in isolation,
yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate
appropriate code solutions is still unexplored. Addressing this gap, we introduce PECC, a novel benchmark
derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems. Unlike conventional
benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate
executable code. A key feature of our dataset is the complexity added by natural language prompting in chat-based
evaluations, mirroring real-world instruction ambiguities. Results show varying model performance between
narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo
passing 50% of the AoC challenges and only 8% on the Euler problems. By probing the limits of LLMsâ€™ capabilities,
our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal
problem solver.
      </p>
      <p>
        <a href="">
          <button><i class="bi bi-file-earmark"></i> Read the Paper</button>
        </a>
      </p>
    </section>

    <section class="shadow-box">
      <figure>
        <!-- <img src="./media/accuracy_by_model_size_bear.svg" width="100%" alt="Illustration of how LM Pub Quiz evaluates LMs." style="max-width: 600px;"> -->
        <figcaption>Accuracy of various models on the BEAR dataset.</figcaption> 
      </figure>
    </section>

    <section class="shadow-box">
      <h2>Example Usage</h2>
      <p>Install the package via pip:</p>
      <pre><code class="language-bash">pip install lm-pub-quiz</code></pre>
      <p>Evaluate a model on BEAR:</p>
      <pre><code class="language-python">from lm_pub_quiz import Dataset, Evaluator

# Load the dataset
bear = Dataset.from_name("BEAR")

# Load the model
evaluator = Evaluator.from_model("gpt2", model_type="CLM", device="cuda:0")

# Run the evaluation
result = evaluator.evaluate_dataset(bear, template_index=0, batch_size=32, save_path="results/gpt2")

# Show the overall accuracy
print(result.get_metrics("accuracy", accumulate_all=True))</code></pre>

      <p>This example script outputs the accuracy accumulated over all relations weighed by the number of instances (this is what we call the "BEAR-score") as a <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html">pandas.Series</a>:</p>
      <pre><code class="language-plaintext">accuracy            0.149528
num_instances    7731.000000
dtype: float64</code></pre>
      <p>For more details, visit the <a href="https://lm-pub-quiz.github.io/lm-pub-quiz/">documentation</a>.</p>
    </section>

    <section class="shadow-box">
      <h2>Citation</h2>
      <p>When using the dataset or library, please cite the following paper:</p>
      <pre><code class="language-plaintext">@misc{wilandBEARUnifiedFramework2024,
title = {{{PECC}}: Problem Extraction and Coding Challenges},
  shorttitle = {{{PECC}}},
  author = {Haller, Patrick and Golde, Jonas and Akbik, Alan},
  year = {2024},
  number = {},
  eprint = {},
  publisher = {arXiv},
  url = {},
}</code></pre>
    </section>

    <section class="contributors-section">
      <h2>Meet the Contributors</h2>
      <div class="contributors-container">
        <div class="shadow-box contributor-card">
          <img src="media/portrait_patrick.jpeg" alt="Portrait of Patrick Haller" class="portrait">
          <h3>Patrick Haller</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://github.com/HallerPatrick/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
        <div class="shadow-box contributor-card">
          <img src="media/portrait_jonas.JPG" alt="Portrait of Jonas Golde" class="portrait">
          <h3>Jonas Golde</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://github.com/whoisjones/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
        <div class="shadow-box contributor-card">
          <img src="media/portrait_alan.jpg" alt="Portrait of Alan Akbik" class="portrait">
          <h3>Alan Akbik</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://alanakbik.github.io/"><i class="bi bi-cursor"></i> Website</a>
            <li><a href="https://github.com/alanakbik/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
      </div>
    </section>
</body>
</html>
