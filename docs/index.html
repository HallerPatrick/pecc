<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PECC</title>

  <meta property="og:title" content="PECC" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://hallerpatrick.github.io/pecc/" />

  <link rel="stylesheet" href="https://fonts.xz.style/serve/inter.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark-dimmed.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

<script>hljs.highlightAll();</script>
</head>
<body>
    <header>
      <div class="title-div">
        <h1><span class="title-lm">PECC</span><br><span class="title-pq"></span></h1>
      </div>
      <div class="subtitle-div">
        <h2 class="subtitle">Problem Extraction and Coding Challenges</h2>
        <nav>
            <a href="https://github.com/HallerPatrick/pecc"><i class="bi bi-git"></i>&nbsp;Library</a> /
            <a href="https://huggingface.co/spaces/PatrickHaller/pecc-leaderboard"<i class="bi bi-bar-chart"></i>&nbsp;Leaderboard</a> /
            <a href="https://huggingface.co/datasets/PatrickHaller/pecc"><i class="bi bi-database"></i>&nbsp;PECC Dataset</a> /
            <a href="https://arxiv.org/abs/2404.18766"><i class="bi bi-file-earmark"></i>&nbsp;Paper</a> 
        </nav>
      </div>
    </header>
    <section class="shadow-box">
        <p style="text-align: center;">
        <b>We introduce PECC:</b> An extensive benchmark centered on code generation from narrative-embedded problem descriptions. Unlike prior benchmarks that evaluate code generation using specific instructions, our dataset requires models to comprehend, extract requirements, and produce the essential code for problem-solving. This approach necessitates syntactically accurate programs and demands reading comprehension skills to derive the desired solution.</p>
    </section>
    <section class="shadow-box">
      <h2>Leaderboard</h2>
      <p>
        To this point, we evaluated 10 competitive large language models (proprietary and open source). Depending on instruction or chat-finetuning of some models a whole evaluation on the AoC dataset subset was not possible.
        We average the accuracy over all 4 subsets.
      </p>
      <div style="overflow-x: scroll;">

<table border="1" class="leaderboard">
  <colgroup>
     <col span="1" style="min-width: 4rem;">
     <col span="1" style="min-width: 3rem;">
     <col span="1" style="min-width: 4rem;">
     <col span="1" style="min-width: 7rem;">
  </colgroup>
  <thead>
    <tr style="text-align: left;">
      <th>Model</th>
      <th>Num Params</th>
      <th>PECC (Pass@3)</th>
    </tr>
  </thead>
  <tbody>


    <tr>
      <td>Claude Haiku</td>
      <td>-</td>
      <td>&#8199;27.67%</td>
    </tr>
    <tr>
      <td>GPT-3.5-Turbo</td>
      <td>-</td>
      <td>&#8199;23.75%</td>
    </tr>
    <tr>
      <td>Codechat-Bison</td>
      <td>-</td>
      <td>&#8199;11.39%</td>
    </tr>
    <tr>
      <td>Chat-Bison</td>
      <td>-</td>
      <td>&#8199;8.48%</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">Mixtral-8x7B-Instruct-v0.1</a></td>
      <td>56B</td>
      <td>&#8199;8.35%</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct">Phi-3-mini-128k-instruct</a></td>
      <td>3.8B</td>
      <td>&#8199;7.18%</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/collections/microsoft/wizardlm-661d403f71e6c8257dbd598a">WizardLM-2-7b</a></td>
      <td>7B</td>
      <td>&#8199;3.72%</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Llama-3-8B-Instruct</a></td>
      <td>8B</td>
      <td>&#8199;3.1%</td>
    </tr>
    <tr>
      <td><a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0">WizardCoder-Python-34B-V1.0</a></td>
      <td>34b</td>
      <td>&#8199;12.9%*</td>
    </tr>
    <tr>
        <td><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1">Mistral-7B-Instruct-v0.1</a></td>
        <td>7B</td>
        <td>&#8199;1.62%*</td>
    </tr>
  </tbody>
</table>
        <p style="text-align: right; font-style: italic;">*Excludes evaluation on Part 2 of AoC subsets</p>

    <section class="shadow-box">
        <p>We compare the PECC scores with commonly used benchmarks for evaluating LLMs. The table below shows the performance
        of the models on the PECC dataset compared to the performance on the average of ARC, MMLU and HellaSwag datasets.</p>
      <figure>
        <img src="./media/plot_scores.png" width="100%" alt="Table of different error types for different models during evaluation" style="max-width: 1000px;">
        <!-- <figcaption>Comparing PECC scores with commonly used benchmarks for evaluating LLMs.</figcaption>  -->
      </figure>
    </section>
      </div>
    </section>
    <section class="shadow-box">
      <div style="text-align: right;"><span class="badge acl">Accepted at LREC-COLING 2024</span></div>
      <h2>Paper Abstract</h2>
      <p>
      Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various
tasks, such as code generation, problem-solving and reasoning. Existing benchmarks evaluate tasks in isolation,
yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate
appropriate code solutions is still unexplored. Addressing this gap, we introduce PECC, a novel benchmark
derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems. Unlike conventional
benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate
executable code. A key feature of our dataset is the complexity added by natural language prompting in chat-based
evaluations, mirroring real-world instruction ambiguities. Results show varying model performance between
narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo
passing 50% of the AoC challenges and only 8% on the Euler problems. By probing the limits of LLMsâ€™ capabilities,
our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal
problem solver.
      </p>
      <p>
        <a href="https://arxiv.org/abs/2404.18766">
          <button><i class="bi bi-file-earmark"></i> Read the Paper</button>
        </a>
      </p>
    </section>

    <section class="shadow-box">
      <figure>
        <img src="./media/error_types.png" width="100%" alt="Table of different error types for different models during evaluation" style="max-width: 600px;">
        <figcaption>Percentage for types of errors during evaluation.</figcaption> 
      </figure>
    </section>

    <section class="shadow-box">
      <h2>Example Usage</h2>
      <p>Clone the repository from Github and setup environment:</p>
      <pre><code class="language-bash">python -m venv venv 
source venv/bin/activate
pip install -r requirements.txt</code></pre>
      <p>Follow instructions on how to download the original AoC subset <a href="https://github.com/HallerPatrick/pecc?tab=readme-ov-file#download-original-aoc-subset">here</a>.</p>
      <p> Run the following script to evaluate a model on PECC</p>
    <pre><code class="language-bash">python main.py --subset euler \
    --output-file gpt3.5-euler-current_results.csv \
    --venv-path venv \
    --model "gpt-3.5-turbo-16k"</code></pre>
      <p>For more details, visit the <a href="https://github.com/HallerPatrick/pecc">documentation</a>.</p>
    </section>

    <section class="shadow-box">
      <h2>Citation</h2>
      <p>When using the dataset or library, please cite the following paper:</p>
    <pre><code class="language-plaintext">@misc{haller2024pecc,
      title={PECC: Problem Extraction and Coding Challenges}, 
      author={Patrick Haller and Jonas Golde and Alan Akbik},
      year={2024},
      eprint={2404.18766},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}</code></pre>
    </section>

    <section class="contributors-section">
      <h2>Meet the Contributors</h2>
      <div class="contributors-container">
        <div class="shadow-box contributor-card">
          <img src="media/portrait_patrick.jpeg" alt="Portrait of Patrick Haller" class="portrait">
          <h3>Patrick Haller</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://hallerpatrick.github.io/"><i class="bi bi-cursor"></i> Website</a>
            <li><a href="https://github.com/HallerPatrick/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
        <div class="shadow-box contributor-card">
          <img src="media/portrait_jonas.JPG" alt="Portrait of Jonas Golde" class="portrait">
          <h3>Jonas Golde</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://github.com/whoisjones/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
        <div class="shadow-box contributor-card">
          <img src="media/portrait_alan.jpg" alt="Portrait of Alan Akbik" class="portrait">
          <h3>Alan Akbik</h3>
          <p class="contributor-role">Core Contributor</p>
          <ul class="contact-info">
            <li><a href="https://alanakbik.github.io/"><i class="bi bi-cursor"></i> Website</a>
            <li><a href="https://github.com/alanakbik/"><i class="bi bi-github"></i> Github Profile</a></li>
          </ul>
        </div>
      </div>
    </section>
</body>
</html>
